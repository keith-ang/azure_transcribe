{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandas transformers spacy torch joblib\n",
    "# spacy.cli.download(\"en_core_web_sm\")  # Ensure the model is downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\intern\\Desktop\\azure_transcribe\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
    "import spacy\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Load sentiment-analysis and zero-shot-classification pipelines with GPU support\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\", device=device)\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=device)\n",
    "\n",
    "# Load spaCy for clarity and complexity checks\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(question):\n",
    "    result = sentiment_pipeline(question)[0]\n",
    "    return result['label'], result['score']\n",
    "\n",
    "def check_relevance(question, topic):\n",
    "    result = classifier(question, [topic])\n",
    "    topic = result['labels'][0]\n",
    "    score = result['scores'][0]\n",
    "    return topic, score\n",
    "\n",
    "def evaluate_clarity(question):\n",
    "    doc = nlp(question)\n",
    "    num_tokens = len(doc)\n",
    "    num_complex_words = sum(1 for token in doc if token.is_alpha and len(token.text) > 6)\n",
    "    return num_tokens, num_complex_words\n",
    "\n",
    "def evaluate_question(question, topic):\n",
    "    sentiment, sentiment_score = analyze_sentiment(question)\n",
    "    topic, relevance_score = check_relevance(question, topic)\n",
    "    num_tokens, num_complex_words = evaluate_clarity(question)\n",
    "    \n",
    "    clarity_score = 1 - (num_complex_words / num_tokens)  # A simple clarity score\n",
    "    effectiveness_score = (relevance_score + clarity_score) / 2\n",
    "    \n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"sentiment\": sentiment,\n",
    "        \"sentiment_score\": sentiment_score,\n",
    "        \"topic\": topic,\n",
    "        \"relevance_score\": relevance_score,\n",
    "        \"num_tokens\": num_tokens,\n",
    "        \"num_complex_words\": num_complex_words,\n",
    "        \"clarity_score\": clarity_score,\n",
    "        \"effectiveness_score\": effectiveness_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the threshold value for filtering\n",
    "THRESHOLD_VALUE = 0.7\n",
    "\n",
    "# Directory containing the JSONL files\n",
    "jsonl_directory = 'coding_jsonl'\n",
    "\n",
    "# List of keywords to exclude\n",
    "exclude_keywords = ['certificate', 'projects', 'rosetta']\n",
    "\n",
    "# Get the list of JSONL files and filter out those with keywords in the name\n",
    "jsonl_files = [\n",
    "    f for f in glob.glob(f\"{jsonl_directory}/*.jsonl\") if not any(keyword in f for keyword in exclude_keywords)\n",
    "]\n",
    "\n",
    "# Output file\n",
    "output_file = 'all_data.jsonl'\n",
    "\n",
    "# Open the output file where we will append valid data\n",
    "output_file_handle = open(output_file, 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] START PROCESSING FILE 1/33: coding_jsonl\\advanced-express-tools.jsonl\n",
      "[INFO] 145/812 questions were evaluated as effective\n",
      "[INFO] FINISH PROCESSING FILE 1/33: coding_jsonl\\advanced-express-tools.jsonl\n",
      "------------------------------------------------------------------------------------\n",
      "\n",
      "[INFO] START PROCESSING FILE 2/33: coding_jsonl\\algorithms.jsonl\n",
      "[INFO] 62/104 questions were evaluated as effective\n",
      "[INFO] FINISH PROCESSING FILE 2/33: coding_jsonl\\algorithms.jsonl\n",
      "------------------------------------------------------------------------------------\n",
      "\n",
      "[INFO] START PROCESSING FILE 3/33: coding_jsonl\\applied-accessibility.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Function to validate the structure of the DataFrame\n",
    "def is_valid_dataframe(df):\n",
    "    if 'messages' not in df.columns:\n",
    "        return False\n",
    "    for messages in df['messages']:\n",
    "        if not isinstance(messages, list):\n",
    "            return False\n",
    "        roles = [message.get('role') for message in messages]\n",
    "        if 'user' not in roles or 'assistant' not in roles:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "n = len(jsonl_files)\n",
    "\n",
    "# Iterate over each JSONL file\n",
    "for i, filename in enumerate(jsonl_files, start=1):\n",
    "\n",
    "    print(f\"[INFO] START PROCESSING FILE {i}/{n}: {filename}\")\n",
    "    topic = os.path.basename(filename).replace('.jsonl', '').replace('-', ' ')\n",
    "\n",
    "    data = []\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                data.append(json.loads(line))\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON in file {filename}: {e}\")\n",
    "                continue\n",
    "\n",
    "    df = pd.json_normalize(data)\n",
    "    \n",
    "    # Validate the DataFrame\n",
    "    if not is_valid_dataframe(df):\n",
    "        print(f\"Skipping file {filename} due to invalid structure.\")\n",
    "        continue\n",
    "\n",
    "    # Extract relevant fields\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for _, row in df.iterrows():\n",
    "        messages = row['messages']\n",
    "        user_message = next((msg['content'] for msg in messages if msg['role'] == 'user'), None)\n",
    "        assistant_message = next((msg['content'] for msg in messages if msg['role'] == 'assistant'), None)\n",
    "        if user_message and assistant_message:\n",
    "            questions.append(user_message)\n",
    "            answers.append(assistant_message)\n",
    "\n",
    "    if len(questions) != len(answers):\n",
    "        print(f\"Inconsistent number of questions and answers in file {filename}. Skipping this file.\")\n",
    "        continue\n",
    "\n",
    "    qna_df = pd.DataFrame({'question': questions, 'answer': answers})\n",
    "\n",
    "    # Evaluate each question sequentially\n",
    "    evaluations = []\n",
    "    for question in qna_df['question']:\n",
    "        evaluations.append(evaluate_question(question, topic))\n",
    "\n",
    "    # Attach evaluations to the DataFrame\n",
    "    qna_df['evaluation'] = evaluations\n",
    "\n",
    "    # Filter rows with effectiveness score greater than THRESHOLD_VALUE\n",
    "    filtered_qna_df = qna_df[qna_df['evaluation'].apply(lambda x: x['effectiveness_score']) > THRESHOLD_VALUE]\n",
    "    print(f\"[INFO] {len(filtered_qna_df)}/{len(qna_df)} questions were evaluated as effective\")\n",
    "\n",
    "    # Append valid Q&A pairs to the output file\n",
    "    for index, row in filtered_qna_df.iterrows():\n",
    "        valid_record = {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert teacher in coding skills for full-stack coding students.\"},\n",
    "                {\"role\": \"user\", \"content\": row['question']},\n",
    "                {\"role\": \"assistant\", \"content\": row['answer']}\n",
    "            ]\n",
    "        }\n",
    "        output_file_handle.write(json.dumps(valid_record) + '\\n')\n",
    "\n",
    "    print(f\"[INFO] FINISH PROCESSING FILE {i}/{n}: {filename}\")\n",
    "    print('------------------------------------------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Close the output file handle\n",
    "# output_file_handle.close()\n",
    "# print(f\"[INFO] Filtered data has been written to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
